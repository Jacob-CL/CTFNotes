# Randomness
- Strictly speaking, there is no such thing as a series of random bits. What IS random is the algo, or process that produces the random bits. So when we say 'random bits' we actually mean randomly generated bits.
- 00000000 or something like 11011000 BOTH have an equal chance of being randomly generated (given high entropy)
- Two common errors:
  - Mistaking nonrandomness for randomness: Thinking that an object was randomly generated simply because it looks random
  - Mistaking randomness for nonrandomness: Thinking that patterns appearing by chance are there for a reason other than chance
  - There's a big difference between random-looking and actually random.
- Any randomized process is characterized / defined by a **probability distribution**
  - A distribution lists the outcomes of a randomized process where each outcome is assigned a probability.
  - A probability measures the likelihood of an event occuring. It's expressed as a real number between 0 and 1. Where 0 = no chance, and 1 = every chance but must include all possible outcomes.
  - A uniform distribution occurs when all probabilities in the distribution are equal, meaning that all outcomes are equally likely to occur.
  - A non-uniform distribution is where the distribution is not equal, e.g weighted/loaded dice
- **Entropy** is the measure of uncertainty, or disorder, in a system. The higher the entropy, the less certainty found in the result of a randomized process.
  - High entropy is good for crypto.
  - Entropy is maximized when the distribution is uniform because a uniform distribution maximizes uncertainty: no outcome is more likely than any other. Just as when the distribtionis not uniform, entropy is lower.
  - Entropy can also be viewed as a measure of information. The result of a fair coin toss = 1 bit of information (heads or tails) and you're unable to predict in advance the result. In an unfair coin toss, you can so the result of the unfair coin toss gives you the info needed to predict the result with certainty.
- **Random and Pseudorandom Number Generators** are components of computers tha return random bits when requested to do so.
  - To do this, you need 2 things: a source of entropy (provided by RNGs) and/or cryptographic algos that produce high quality random bits (found in PRNGs) but using both RNGs and PRNGs is the key to making cryptography practical and secure.
  - For **RNGs** randomness comes from the environment (leverage analog entropy like temperature, air pressure to produce unpredictable bits in a digital system.) but sometimes can be difficult to estimate the entropy of these environment factors to generate entropy. They can also harvest the entropy in a running OS by drawing from attached sensors, I/O devices, network or disk activity, running process, mouse movements etc
  - Such system- and human-generated activities can be a good source of entropy, but they can be fragile and manipulated by an attacker. Also, they’re slow to yield random bits.
  - Such a thing as QRNG which is a type of RNG that relies on the randomness arising from quantum mechanical phenomena such as radioactive decay, photon polirazation or thermal noise.
  - For **PRNG**, they address the challenge in generating randmonness by reliably producing many artificial random bits from a few true random bits. e.g an RNG system that translates mouse movements to random bits would stop working if you stopped moving the mouse, whereas a PRNG always returns pseudorandom bits when requested to do so.
  - PRNG rely RNGs but behave differently.. RNGs produce random bits relatively slowly from analog sources, in a non-deterministic way, and with no guarantee of uniform distribution or of high entropy. In contrast, PRNGs produce random-looking bits quickly from digital sources, in a deterministic way, uniformly distributed, and with an entropy guaranteed to be high enough for cryptography.
  - Essentially, PRNGs transform a few unreliable random bits into long term stream of reliable pseudorandom bits suitable for crypto applications.
